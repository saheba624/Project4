<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Manage JMX credentials on Kubernetes with Cryostat 2.1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/19/manage-jmx-credentials-kubernetes-cryostat-21" /><author><name>Janelle Law</name></author><id>2fd872df-08bd-421a-a2b6-cc0f3e6ba997</id><updated>2022-05-19T07:00:00Z</updated><published>2022-05-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; is a tool for managing &lt;a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170"&gt;JDK Flight Recorder&lt;/a&gt; data on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. If you have &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; Management Extensions (JMX) authentication enabled on your &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; Java Virtual Machines (JVMs), Cryostat will prompt you to enter your JMX credentials before it can access the JDK flight recordings on your target JVMs. On the Cryostat console, the &lt;strong&gt;Automated Rules, Recordings,&lt;/strong&gt; and &lt;strong&gt;Events&lt;/strong&gt; tabs will require you to enter your JMX credentials if you want to view existing flight recordings or perform a recording operation on a target with JMX authentication enabled. When monitoring multiple target JVMs with Cryostat features such as &lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui#"&gt;automatic rules&lt;/a&gt;, you may want Cryostat to remember and reuse your JMX credentials for each target connection.&lt;/p&gt; &lt;p&gt;Cryostat stores JMX credentials according to each target's unique JMX service URL, also known as a &lt;em&gt;connection URL.&lt;/em&gt; Even if the underlying JVM instance changes, the target alias changes, or the target application restarts, Cryostat will apply the stored JMX credentials to the connection URL that the credentials are associated with.&lt;/p&gt; &lt;p&gt;If you would like to start an automated rule to automatically start and save recordings on your target applications, you will need to store credentials for each of your selected targets with the &lt;strong&gt;Security&lt;/strong&gt; tab prior to creating the rule. If Cryostat is missing credentials for a target requiring JMX authentication, the rule will be unable to connect to the target JVM and will not start a recording.&lt;/p&gt; &lt;p&gt;Here's how to manage stored JMX credentials with the Cryostat web UI.&lt;/p&gt; &lt;h2&gt;How to store JMX credentials in Cryostat&lt;/h2&gt; &lt;p&gt;First, navigate to the &lt;strong&gt;Security&lt;/strong&gt; tab. The Stored Credentials table lists all targets for which Cryostat has stored JMX credentials. Click &lt;strong&gt;Add&lt;/strong&gt; as shown in Figure 1 to enter JMX credentials for your desired target JVM.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img src="https://developers.redhat.com/sites/default/files/empty-jmx-credentials-ui.png" width="1920" height="972" alt="An empty Stored JMX Credentials table on the Security tab in the Cryostat web UI. " loading="lazy" typeof="Image" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Navigate to the Security tab to store JMX credentials.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;A modal will appear, prompting you to select a target JVM and enter your JMX credentials, as shown in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img src="https://developers.redhat.com/sites/default/files/store-jmx-credentials.png" width="1920" height="972" alt="Selecting a target JVM and entering a username and password on the modal form." loading="lazy" typeof="Image" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Enter your JMX credentials.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Alternatively, JMX credentials will be automatically stored if you navigate to either the &lt;strong&gt;Recordings&lt;/strong&gt; or &lt;strong&gt;Events&lt;/strong&gt; tab and select a target JVM with JMX authentication enabled, as shown in Figure 3. A similar authentication form will appear, prompting you to enter your JMX credentials. The credentials will be automatically stored and will appear in the Stored JMX Credentials table. Your credentials will be remembered automatically, and you can delete them at any time.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img src="https://developers.redhat.com/sites/default/files/navigate-to-recordings-1.png" width="1920" height="971" alt="When navigating to the Recordings tab and selecting a target JVM for which Cryostat does not have stored credentials, an authentication modal form appears for you to store JMX credentials. " loading="lazy" typeof="Image" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: This JMX authentication popup will appear when you need to authenticate before viewing recordings for a target JVM.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;When you store credentials for a target, the target alias and connect URL will appear on the Stored Credentials table in the &lt;strong&gt;Security&lt;/strong&gt; tab to indicate that Cryostat has stored JMX credentials for that target as shown in Figure 4. Again, only the connect URL matters when associating stored credentials for a particular target. You are free to change the target alias at any time without affecting Cryostat's ability to locate or apply stored JMX credentials to your target JVMs.&lt;/p&gt; &lt;p&gt;As a security precaution, you will not be able to view the actual credentials after you have submitted them. If you would like to replace the stored credentials for an existing target, you can delete the old credentials entry and add a new entry with the same connect URL as the old entry. To remove any stored credentials, select the checkbox next to the target and click &lt;strong&gt;Delete.&lt;/strong&gt; To delete all stored JMX credentials, select the header checkbox at the top of the table and click &lt;strong&gt;Delete.&lt;/strong&gt;&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img src="https://developers.redhat.com/sites/default/files/view-store-credential-target-table_0.png" width="1920" height="934" alt="The stored credentials table shows the target alias and connect url for the target JVM." loading="lazy" typeof="Image" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Stored JMX credentials table.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article covers how to store JMX credentials for Cryostat to reuse when connecting to containerized JVMs. For more information about Cryostat, visit &lt;a href="https://cryostat.io/get-started/"&gt;cryostat.io&lt;/a&gt;. For questions, comments, and feedback, feel free to connect with us on &lt;a href="https://github.com/cryostatio"&gt;GitHub&lt;/a&gt; or join our &lt;a href="https://groups.google.com/g/cryostat-development"&gt;mailing list&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/19/manage-jmx-credentials-kubernetes-cryostat-21" title="Manage JMX credentials on Kubernetes with Cryostat 2.1"&gt;Manage JMX credentials on Kubernetes with Cryostat 2.1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Janelle Law</dc:creator><dc:date>2022-05-19T07:00:00Z</dc:date></entry><entry><title type="html">WildFly 26.1.1 is released!</title><link rel="alternate" href="https://wildfly.org//news/2022/05/19/WildFly2611-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2022/05/19/WildFly2611-Released/</id><updated>2022-05-19T00:00:00Z</updated><content type="html">WildFly 26.1.1.Final is now available . It’s been about five weeks since the WildFly 26.1 release, so we’ve done a small bug fix update, WildFly 26.1.1. This includes an update to WildFly Preview. The full list of issues resolved in WildFly 26.1.1 is available . Issues resolved in the WildFly Core update included with WildFly 26.1.1 are available . Note that the release artifacts are available in public maven in the but at the time of writing we’re aware of some issues with syncing artifacts from there to Maven Central, so there may be some delays before the 26.1.1.Final artifacts can be resolved from there. Enjoy!</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>What's new in Red Hat Enterprise Linux 9</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9" /><author><name>Nikhil Mungale, Alex Krikos</name></author><id>d0a214a1-88df-4b37-94ab-3b629343dbb6</id><updated>2022-05-18T13:10:00Z</updated><published>2022-05-18T13:10:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 9 is now generally available (GA). This release is designed to meet the needs of the hybrid cloud environment, and is ready for you to develop and deploy from the edge to the cloud. It can run your code efficiently whether deployed on physical infrastructure, in a virtual machine, or in &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; built from &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBIs).&lt;/p&gt; &lt;p&gt;RHEL 9 can be &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;downloaded for free&lt;/a&gt; as part of the Red Hat Developer program subscription. In this article, you'll learn some of the ways that RHEL 9 can improve the developer experience.&lt;/p&gt; &lt;h2&gt;Get access to the latest language runtimes and tools&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Red Hat Enterprise Linux 9 is built with a number of the latest runtimes and &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compilers&lt;/a&gt;, including GCC 11.2 and updated versions of LLVM (12.0.1), Rust (1.54.0), and &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; (1.16.6), enabling developers to modernize their applications.&lt;/li&gt; &lt;li&gt;RHEL 9 ships with updated versions of core developer toolchains such as GCC (11.2), glibc (2.34), and binutils (2.35). The new features in the GCC compiler help users better track code flow, improve debugging options, and write optimized code for efficient hardware usage. The new GCC compiler comes with modifications for &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt; code compilation, along with new debugging messages for logs. That gives developers a better handle on how their code performs.&lt;/li&gt; &lt;li&gt;With next-generation &lt;a href="https://developers.redhat.com/blog/2018/11/15/rhel8-introducing-appstreams"&gt;application streams&lt;/a&gt;, developers will have more choices when it comes to versions of popular languages and tools. Red Hat Enterprise Linux 9 improves the application streams experience by providing initial application stream versions that can be installed as RPM packages using the traditional &lt;code&gt;yum&lt;/code&gt; install command. Developers can select from multiple versions of user-space components as application streams that are easy to update, providing greater flexibility to customize RHEL for their development environment. Application stream contents also include tools and applications that move very fast and are updated frequently. These application streams, called &lt;em&gt;rolling streams,&lt;/em&gt; are fully supported for the full life of RHEL 9.&lt;/li&gt; &lt;li&gt;Red Hat Enterprise Linux 9 extends RHEL 8's module packaging features. With RHEL 9, all packaging methods, such as &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt;, &lt;a href="https://developers.redhat.com/blog/2020/08/12/introducing-the-red-hat-flatpak-runtime-for-desktop-containers"&gt;Flatpaks&lt;/a&gt;, and traditional RPMs, have been incorporated into application streams, making it easier for developers to use their preferred packages.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Support for newer versions of language runtimes&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; 3.9 gets lifetime support in Red Hat Enterprise Linux 9 and comes with a host of new features, including timezone-aware timestamps, new string prefix and suffix methods, dictionary union operations, high-performance parsers, multiprocessing improvements, and more. These features will help developers modernize their applications easily.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; 6 provides changes that include an upgrade to the V8 engine to version 9.2, a new Timer Promises API, a new experimental web streams API, and support for npm package manager version 7.20.3. Node.js is now compatible with OpenSSL 3.0.&lt;/li&gt; &lt;li&gt;Ruby 3.0.2 provides several performance improvements, along with bug and security fixes. Some of the important improvements include concurrency and parallelism, static analysis, pattern matching with &lt;code&gt;case&lt;/code&gt;/&lt;code&gt;in&lt;/code&gt; expressions, redesigned one-line pattern matching, and find pattern matching.&lt;/li&gt; &lt;li&gt;Perl 5.32 provides a number of bug fixes and enhancements, including Unicode version 13, a new experimental infix operator, faster feature checks, and more.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/php"&gt;PHP&lt;/a&gt; 8.0 provides several bug fixes and enhancements, such as the use of structured metadata syntax, newly named arguments that are order-independent, improved performance for Just-In-Time compilation, and more.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Build Red Hat Enterprise Linux images for development and testing&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/blog/using-no-cost-developer-subscription-new-red-hat-enterprise-linux-image-builder-hosted-service"&gt;Image builder&lt;/a&gt; is a tool that allows users to create custom RHEL system images in a variety of formats for major and minor releases. These images are compatible with major cloud providers and virtualization technologies popular in the market. This enables users to quickly spin up customized RHEL development environments on local, on-premise, or cloud platforms.&lt;/p&gt; &lt;p&gt;With image builder, custom filesystem configurations can be specified in blueprints to create images with a specific disk layout, instead of using the default layout configuration.&lt;/p&gt; &lt;p&gt;Image builder can be used to create bootable ISO installer images. These images consist of a tarball that contains a root filesystem that you can use to install directly to a bare metal server, which is ideal for bringing up test hardware for edge developments.&lt;/p&gt; &lt;h2&gt;Monitor and maintain Red Hat Enterprise Linux environments&lt;/h2&gt; &lt;p&gt;The Red Hat Enterprise Linux 9 web console has an enhanced performance metrics page that helps identify potential causes of high CPU, memory, disk, and network resource usage spikes. In addition, subsystem metrics can be easily exported to a Grafana server.&lt;/p&gt; &lt;p&gt;RHEL 9 also now supports kernel live patching via the web console. The latest critical kernel security patches and updates can be applied immediately without any need for scheduled downtime, and without disrupting ongoing development or production applications.&lt;/p&gt; &lt;h2&gt;Build containers with Universal Base Images&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9 ships with control groups (cgroup) and a recent release of Podman with improved defaults. Signature and container short-name validation are enabled by default and containerized applications can be tested on the out-of-the-box RHEL 9 configuration.&lt;/p&gt; &lt;p&gt;The RHEL 9 UBI is available in standard, micro, minimal or init image configurations, which range in size from as small as 7.5MB up to 80MB. Learn more about &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/building_running_and_managing_containers/index"&gt;how to build, run, and manage containers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Identity and security&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;With Red Hat Enterprise Linux 9, root user authentication with a password over SSH has been disabled by default. The OpenSSH default configuration disallows root user login with a password, thereby preventing attackers from gaining access through brute-force password attacks. Instead of using the root password, developers can access remote development environments using SSH keys to log in.&lt;/li&gt; &lt;li&gt;OpenSSL 3.0 adds a provider concept, a new versioning scheme, and an improved HTTPS. &lt;em&gt;Providers&lt;/em&gt; are collections of algorithm implementations. Developers can programmatically invoke any providers based on application requirements. Built-in RHEL utilities have been recompiled to utilize OpenSSL 3. This allows users to take advantage of new security ciphers for encrypting and protecting information.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;More resources&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Begin developing now on Red Hat Enterprise Linux 9 for free. See the &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Red Hat Enterprise Linux product page&lt;/a&gt; for downloads and more.&lt;/li&gt; &lt;li&gt;Read the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/9.0_release_notes/index"&gt;RHEL 9 release notes&lt;/a&gt; for more details.&lt;/li&gt; &lt;li&gt;Read the &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-defines-new-epicenter-innovation-red-hat-enterprise-linux-9"&gt;corporate press release for the RHEL 9 GA&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9" title="What's new in Red Hat Enterprise Linux 9"&gt;What's new in Red Hat Enterprise Linux 9&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale, Alex Krikos</dc:creator><dc:date>2022-05-18T13:10:00Z</dc:date></entry><entry><title>A SaaS architecture checklist for Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes" /><author><name>Michael Hrivnak, Alex Kubacki, Mike Guerette, Rob Terzi</name></author><id>dabec593-28d2-4528-abef-debad18ed90d</id><updated>2022-05-18T07:00:00Z</updated><published>2022-05-18T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first in a series of articles about building and deploying &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-saas"&gt;software as a service (SaaS)&lt;/a&gt; applications, which will focus on software and deployment architectures. The topics the series will cover can be used as the basis for a checklist for SaaS architecture. These topics include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Approaches to providing &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-multitenancy"&gt;multitenancy&lt;/a&gt; in SaaS applications&lt;/li&gt; &lt;li&gt;Security controls and considerations for SaaS&lt;/li&gt; &lt;li&gt;Approaches to storing and managing tenants' persistent data&lt;/li&gt; &lt;li&gt;Build and deployment pipelines and &lt;a href="https://developers.redhat.com/topics/automation/all"&gt;automation&lt;/a&gt; for SaaS development&lt;/li&gt; &lt;li&gt;Scalability, high availability, and disaster recovery for SaaS deployments&lt;/li&gt; &lt;li&gt;Options for deploying existing non-containerized applications in a SaaS environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As we publish articles in this series, we will update this article and provide the links to them. Bookmark this article for easy access to the complete set.&lt;/p&gt; &lt;p&gt;Throughout the series, we will highlight technologies that can be used to help you build and deploy SaaS applications. In particular, we will discuss best practices and the advantages that a container orchestration platform like &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; have for running SaaS applications.&lt;/p&gt; &lt;p&gt;This article provides background information on SaaS. Those unfamiliar with Kubernetes will also get a brief introduction to that platform to prepare you for the rest of this series.&lt;/p&gt; &lt;h2&gt;What is SaaS?&lt;/h2&gt; &lt;p&gt;From the end users' perspective, the defining characteristic of a SaaS application is that it's on-demand software typically delivered through the web, so the users only need a browser or mobile device to access it. The provider of the SaaS application manages all the software and infrastructure needed to deliver the application service to the consumers of the application.&lt;/p&gt; &lt;p&gt;SaaS applications are economically attractive because the development, infrastructure, and support costs are shared by multiple customers of the service. Consumers can start using the application without capital expenditures or waiting for the software to be installed. Subscription-based pricing lowers the risk for consumers, which results in shortened sales cycles for SaaS providers. The recurring revenue stream from subscriptions can be invested into the application to grow market share.&lt;/p&gt; &lt;p&gt;Agility is a significant advantage that SaaS providers have over traditional software vendors. SaaS consumers have nearly immediate access to new releases, allowing SaaS providers to iterate quickly. The ability to deliver new features to market quickly can be a competitive edge.&lt;/p&gt; &lt;p&gt;Because SaaS applications are shared, their consumers are referred to as &lt;em&gt;tenants.&lt;/em&gt; A tenant could either be a single user or a collection of users who belong to the same organization. Each tenant has a view that shows that they are using a private resource. Even though they might be sharing hardware or software instances, other tenants' use of the system is usually completely hidden from view. This is referred to as &lt;em&gt;multitenancy.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Red Hat OpenShift provides many capabilities to make SaaS deployments easier. It is a platform-as-a-service (PaaS) that builds on Kubernetes' capabilities to orchestrate multiple workloads and provide rapid scaling—capabilities that have made Kubernetes the de facto standard for cloud-native applications. Automated build and deployment pipelines, &lt;a href="https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator"&gt;Kubernetes Operators&lt;/a&gt;, and the ability to manage &lt;a href="https://www.redhat.com/en/topics/automation/what-is-infrastructure-as-code-iac"&gt;infrastructure as code&lt;/a&gt; are just some of the features that make OpenShift ideal for building and deploying SaaS solutions.&lt;/p&gt; &lt;h2&gt;What Kubernetes concepts are important for SaaS?&lt;/h2&gt; &lt;p&gt;For the discussion of SaaS architecture in this series, it is helpful to understand a few relevant terms and concepts related to Kubernetes and Red Hat OpenShift. In this series, any discussion of Kubernetes will also apply to Red Hat OpenShift unless explicitly noted, since Red Hat OpenShift is an enterprise Kubernetes platform.&lt;/p&gt; &lt;p&gt;A Kubernetes &lt;em&gt;cluster&lt;/em&gt; is a group of hosts working together to run Linux containers in an orchestrated fashion. The Kubernetes concepts relevant to SaaS architecture include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Control plane: &lt;/strong&gt;The collection of processes that manage the Kubernetes cluster and allocate work to the hosts in that cluster. For a production environment, the control plane typically runs on three dedicated nodes to provide high availability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Worker nodes: &lt;/strong&gt;These are the hosts that run the application workloads in the cluster. The control plane assigns work to these nodes in the form of &lt;em&gt;pods.&lt;/em&gt; The number of worker node hosts in the cluster is determined by the anticipated or observed load of the applications that run on the cluster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pod:&lt;/strong&gt; A group of one or more &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; running on a single node. The containers running in the pod can be the application itself or application components such as a web server. A node can run many unrelated pods or multiple instances of the same pod as necessary to handle the load.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployments and workload management:&lt;/strong&gt; Kubernetes provides a number of flexible mechanisms, including &lt;a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes-deployment"&gt;deployments&lt;/a&gt; and stateful sets, for declaring which workloads should be running and how they should be managed by the cluster's control plane. A deployment can be used to declare that a specific number of pods running the application's web server are required. It is then up to Kubernetes to decide which nodes to run those pods on and replace them if either a pod or node fails.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Services:&lt;/strong&gt; Services within Kubernetes provide a mechanism to access an application component, such as a web service, running in one or more pods while avoiding any coupling to a specific pod or pods. Pods are ephemeral; they can be created and destroyed as necessary for failover and scaling. The abstraction of the network from services allows Kubernetes to route requests and provide load balancing between pods.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The components of a Kubernetes cluster are shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kubernetes_diagram-v3-770x717_0_0_v2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kubernetes_diagram-v3-770x717_0_0_v2.png?itok=zq0WmAQc" width="600" height="559" alt="Diagram showing the components of a Kubernetes cluster" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The components of a Kubernetes cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Pods abstract network and storage away from the underlying containers. This allows pods to move to other worker nodes in the cluster as needed. The containers in a pod share a virtual IP address, hostname, and other resources that allow the containers within a pod to communicate with each other.&lt;/p&gt; &lt;p&gt;To allow pods to be easily moved and replicated, network services such as web servers are exposed outside of a pod by defining Kubernetes services. Kubernetes is responsible for routing any service requests to a running pod that can handle them, no matter where it is running in the cluster. In addition to allowing pods to move between worker nodes, using services also allows scaling by providing the ability to route and balance requests to multiple pods providing the same service.&lt;/p&gt; &lt;p&gt;For more information on Kubernetes concepts, refer to Red Hat's &lt;a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes"&gt;What is Kubernetes?&lt;/a&gt; article.&lt;/p&gt; &lt;h2&gt;A look ahead&lt;/h2&gt; &lt;p&gt;The next articles in this SaaS architecture series will cover approaches to multitenancy, security, and the options available for creating a defense-in-depth security strategy. The full list of articles will remain at the top of this article.&lt;/p&gt; &lt;p&gt;Red Hat SaaS Foundations is a partner program designed for building enterprise-grade SaaS solutions on the &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; or &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; platforms and deploying them across multiple cloud and non-cloud footprints. &lt;a href="http://mailto:saas@redhat.com"&gt;Email&lt;/a&gt; us to learn more about partnering with Red Hat to build your SaaS.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes" title="A SaaS architecture checklist for Kubernetes"&gt;A SaaS architecture checklist for Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Hrivnak, Alex Kubacki, Mike Guerette, Rob Terzi</dc:creator><dc:date>2022-05-18T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.21.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/05/kogito-1-21-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/05/kogito-1-21-0-released.html</id><updated>2022-05-17T11:06:25Z</updated><content type="html">We are glad to announce that the Kogito 1.21.0 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * New source files add-on that allows you to download the source files (.bpmn, .bpmn2, .json, and .yaml). * New Jobs integration add-on based on Messaging / Kafka * OAuth2 support on Serveless Workflow OpenAPI BREAKING CHANGES * Persistence add-ons for Spring Boot runtime have been updated with a new Maven Artifact Id, for more details see . * Codegen maven step should be disable or there is a likely chance openapi in SWF will stop working (Quarkiverse integration is enabled with codegen maven step and is still in experimental phase, some specs are working, but others not) For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.19.0 artifacts are available at the . A detailed changelog for 1.21.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Manage JFR across instances with Cryostat and GraphQL</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql" /><author><name>Andrew Azores</name></author><id>2b8cc495-858d-4cd6-b5e4-f140f6b8684e</id><updated>2022-05-17T07:00:00Z</updated><published>2022-05-17T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; manages the monitoring of &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; applications using &lt;a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170"&gt;Java Flight Recorder&lt;/a&gt; (JFR) in the cloud. Cryostat 2.1 includes support for &lt;a href="https://graphql.org"&gt;GraphQL&lt;/a&gt; to control flight recordings on multiple applications, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; pods, with powerful filtering capacities. This article discusses the motivation for adding GraphQL support, shares some examples of queries along with expected results, and takes a look at the underlying web requests on the GraphQL endpoint.&lt;/p&gt; &lt;h2&gt;Why use GraphQL with Cryostat?&lt;/h2&gt; &lt;p&gt;Previous versions of Cryostat exposed flight recordings with a simple HTTP REST API that limited each request to a single conceptual action—starting one recording on one replica instance, for example. But Cryostat serves developers who create &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; architectures running large numbers of instances in multiple scaled replicas, and each replicated instance can have many flight recordings running for various purposes.&lt;/p&gt; &lt;p&gt;Thus, a developer who wanted to start an identical recording on 12 replicas of a simple container needed to make 12 API requests. Even worse, a developer starting an identical recording on 10 replicas across 10 microservices—a common use case—needed to make 10x10=100 API requests.&lt;/p&gt; &lt;p&gt;Clearly, Cryostat needs a flexible and powerful API for managing all these conditions. GraphQL is a simple but capable language for issuing queries on groups of carefully chosen instances. Reducing multiple actions to a single API request improves overall performance significantly, due to reduced network traffic overhead. As an additional benefit, developers can write much shorter and simpler queries to perform complex actions, rather than writing custom clients that parse API JSON responses and perform actions iteratively on response data.&lt;/p&gt; &lt;p&gt;Queries for target JVMs and the active or archived recordings that belong to them, as well as recordings present in the general Cryostat archives, can combine with mutations to start, stop, archive, and delete active or archived recordings to create powerful queries for building automation around Cryostat and JDK Flight Recorder.&lt;/p&gt; &lt;h2&gt;Example queries&lt;/h2&gt; &lt;p&gt;Cryostat accepts queries formatted in GraphQL at its &lt;code&gt;/api/beta/graphql&lt;/code&gt; endpoint. If you are completely unfamiliar with GraphQL, I suggest you take a brief look at &lt;a href="https://graphql.org/learn/"&gt;its documentation&lt;/a&gt; to gain a better understanding of which parts in the examples in this article are just GraphQL syntax and concepts, versus Cryostat-specific behavior.&lt;/p&gt; &lt;p&gt;Here is our first simple query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;query { targetNodes { name nodeType labels target { alias serviceUri } } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This asks Cryostat for the &lt;code&gt;targetNodes&lt;/code&gt; result, which is a query that returns all of the JVM targets that Cryostat is aware of. The response is an array of those objects. Since the query specifies various fields like &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;nodeType&lt;/code&gt;, the objects within the array contain only those fields.&lt;/p&gt; &lt;p&gt;Here is another, more advanced query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;query { environmentNodes(filter: { name: "my-app-pod-abcd1234" }) { descendantTargets { doStartRecording(recording: { name: "myrecording", template: "Profiling", templateType: "TARGET", duration: 30 }) { name state } } } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This asks Cryostat for the &lt;code&gt;environmentNodes&lt;/code&gt; result, which is a query that returns all of the nodes in the deployment graph that Cryostat is aware of but that are &lt;em&gt;not&lt;/em&gt; JVM target applications. In a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; context, results would include the &lt;code&gt;Deployments&lt;/code&gt; or &lt;code&gt;DeploymentConfigs&lt;/code&gt;, for example, and the &lt;code&gt;Pods&lt;/code&gt; that belong to them.&lt;/p&gt; &lt;p&gt;The previous query used a name-based filter, selecting only the nodes with the name &lt;code&gt;my-app-pod-abcd1234&lt;/code&gt;. Let's assume that this string matches one pod within our project namespace. Upon that pod, the query performs the nested query &lt;code&gt;descendantTargets&lt;/code&gt;, which yields an array of JVM target objects much like our previous example query.&lt;/p&gt; &lt;p&gt;Upon each of those JVM targets, we perform the &lt;code&gt;doStartRecording&lt;/code&gt; mutation. As its name suggests, this mutation causes Cryostat to start a new JDK Flight Recording on each of the JVM targets using the configuration provided. The response will contain simply the name and state of each of these recordings, which we can anticipate to be &lt;code&gt;myrecording&lt;/code&gt; and &lt;code&gt;RUNNING&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The previous example shows the power and utility of the GraphQL API, where a single request can make Cryostat perform the complex action of communicating with OpenShift to determine all of the Cryostat-compatible JVMs that belong to the specific pod and start a recording on each JVM.&lt;/p&gt; &lt;p&gt;Here is one final sample query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;query { targetNodes(filter: { annotations: "PORT = 9093" }) { recordings { active(filter: { labels: "mylabel = redhatdevelopers" }) { doArchive { name } } } } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If you are familiar with OpenShift, the &lt;code&gt;labels&lt;/code&gt; filter here might look familiar to you. This label selector uses the same syntax as OpenShift's label selectors, and applies to Cryostat's &lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21"&gt;recording labels&lt;/a&gt; as well as to the labels and annotations in the deployment graph.&lt;/p&gt; &lt;p&gt;You can use expressions like &lt;code&gt;mylabel = redhatdevelopers&lt;/code&gt;, &lt;code&gt;env in (prod, stage)&lt;/code&gt;, or &lt;code&gt;!mylabel&lt;/code&gt;. You can also create a logical AND conjunction of multiple label selectors by passing them as an array in the filter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-graphql"&gt;active(filter: { labels: ["mylabel = redhatdevelopers", "env in (prod, stage)"] })&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;To better understand what Cryostat sees in your deployment graph for your particular namespace, check the &lt;a href="https://github.com/cryostatio/cryostat/blob/cryostat-v2.1/HTTP_API.md#DiscoveryGetHandler"&gt;Discover API endpoint&lt;/a&gt; with a command like:&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl https://my-cryostat.openshift.example.com/api/v2.1/discovery | jq&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;There are several different kinds of filters, which can be combined and applied to various nested queries. So a large number of possible semantic requests can be pieced together. Refer to Cryostat's GraphQL &lt;a href="https://github.com/cryostatio/cryostat/blob/cryostat-v2.1/src/main/resources/queries.graphqls"&gt;queries.graphqls&lt;/a&gt; and &lt;a href="https://github.com/cryostatio/cryostat/blob/cryostat-v2.1/src/main/resources/types.graphqls"&gt;types.graphqls&lt;/a&gt; repositories to see all of the implemented queries and mutations, and learn which ones accept which kinds of filters. The names should be self-explanatory: Types ending in &lt;code&gt;FilterInput&lt;/code&gt; are filters, and type fields starting with &lt;code&gt;do&lt;/code&gt; are actually nested mutations that perform some action like starting a recording or copying a recording to the archives.&lt;/p&gt; &lt;h2&gt;API endpoint details&lt;/h2&gt; &lt;p&gt;The examples in this article show typical query payloads of a GraphQL endpoint, the behaviors that you can perform, and what the expected responses should be. In this section, we take a closer look at the specific details of how to make these requests.&lt;/p&gt; &lt;p&gt;The primary GraphQL endpoint for Cryostat 2.1 is &lt;code&gt;POST /api/beta/graphql&lt;/code&gt;. The &lt;code&gt;POST&lt;/code&gt; requests sent to this path include a body string formatted like &lt;code&gt;{ query: "THE_QUERY" }&lt;/code&gt;, replacing &lt;code&gt;THE_QUERY&lt;/code&gt; with the text of the query as given in the previous section's examples. Here's a concrete example of a request and its metadata:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;POST /api/beta/graphql HTTP/1.1 Accept: application/json, */*;q=0.5 Accept-Encoding: gzip, deflate Connection: keep-alive Content-Length: 171 Content-Type: application/json Host: localhost:8181 User-Agent: HTTPie/3.1.0 { "query": "query {\n targetNodes {\n name\n nodeType\n labels\n target {\n alias\n serviceUri\n }\n }\n}\n" }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can also use a &lt;code&gt;GET /api/beta/graphql&lt;/code&gt; request with the same results:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;"/api/beta/graphql?query={targetNodes{name nodeType labels target{alias serviceUri}}}" HTTP/1.1 200 OK content-encoding: gzip content-length: 247 content-type: application/json&lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Taking GraphQL further&lt;/h2&gt; &lt;p&gt;In the previous section's examples, we explored some possible query filters. To play with the power of the GraphQL interface in the context of your environment, try the following exercise:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Craft a GraphQL query that snapshots all of your JVM applications at once and copies the resulting recordings into the Cryostat archives.&lt;/li&gt; &lt;li&gt;Execute your query by sending the request to Cryostat using cURL or your favorite HTTP client.&lt;/li&gt; &lt;li&gt;Write a script wrapping around this API request and add it to your crontab to run at 5:00 PM every day.&lt;/li&gt; &lt;li&gt;Combine this script with a &lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;Cryostat automated rule&lt;/a&gt; that automatically starts a new continuous monitoring recording on your JVM applications whenever they appear.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Each of these pieces is simple in isolation, but together they form a powerful automation workflow.&lt;/p&gt; &lt;p&gt;One final note: The standard API in older Cryostat versions still exists in 2.1, and there are even further additions to this API for actions and capabilities that are not suitable for GraphQL—new endpoints for downloading JDK Flight Recorder files using JWT tokens rather than Authorization headers.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql" title="Manage JFR across instances with Cryostat and GraphQL"&gt;Manage JFR across instances with Cryostat and GraphQL&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores</dc:creator><dc:date>2022-05-17T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.9.1.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-9-1-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-9-1-final-released/</id><updated>2022-05-17T00:00:00Z</updated><content type="html">It is my pleasure to announce the availability of Quarkus 2.9.1.Final, the first maintenance release of our 2.9 release. It is a safe upgrade for anyone already using 2.9. If you are not using 2.8 already, please refer to the 2.9 migration guide. Full changelog You can get the full...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">DMN Types from Java Classes</title><link rel="alternate" href="https://blog.kie.org/2022/05/dmn-types-from-java-classes.html" /><author><name>Yeser Amer</name></author><id>https://blog.kie.org/2022/05/dmn-types-from-java-classes.html</id><updated>2022-05-16T18:48:05Z</updated><content type="html">Drawing a real case DMN asset may become a time-consuming activity. In some domains, the possible types involved in DMN logic can explode into dozens or even hundreds of possible involved objects. Although a well-designed UI can support users to define your domain object type in a simpler and faster way possible, other alternative strategies are a practical choice for the designer, in cases where a large domain is in place. This is one of the reasons, we designed and implemented a new feature in the DMN Editor: Importing Java Beans and translating them as DMN Type. Suppose you are a regular reader of the KIE Blog. In that case, you should already know our tooling editors offer developer-centric features, which place the developer in the center of the feature experience. In a few words, the scope of this goal relies on task automation, testing management, and reuse. The Import of Java Classes feature is one of these kinds of functionalities, and in detail: * Reducing time to define your DMN assets: A developer can write hundred of Java classes faster than defining them in the UI; * Domain usage: In some cases, your domain can be already available and well defined with Java Classes * Class management and Type safety: The enablement of the feature, the actual classes you can import into your project, and the type safety of your assets are manageable aspects the user can rely on. In this post, we will guide you in using this new feature! TUTORIAL Here are the required elements for this guide: * (1.46.0+); * (1.22.0+) *   or (0.19.0+); * (1.6.0+); * Your domain’s Java Beans; * The Activator, a Java class file required to activate the functionality; However, I strongly suggest using the latest versions of the above elements. PROJECT SETUP Let’s start laying the foundation of our project. Let’s consider a Kogito-based project present in our . Our pick-up is the , which already contains DMN assets. So, clone that module in your local. The next step is to add some Java Bean classes, which describe your own domain. In our case, we will use Book and Author classes. Our dmn-quarkus-example project An important point to highlight is your Java Classes must be Java Beans and, in detail, they must have public getters of their internal fields. This means if a field of your Java Class is private or without a public getter method, our functionality will be not able to find that field. An alternative is to set that field with the public identifier, but we strongly recommend referring to the Java Beans specification to use this feature. The second point to analyze is the required Activator Class (no matter the class name). This is a class that contains a annotation, and it’s essential for the correct behavior of the functionality. This annotation is available in the latest version of Kogito, that’s the reason we chose (and recommend) to use a kogito-based project. In case you need to directly import the annotation, just import in your maven project the kie-api dependency, with at least version 8.22.0.Beta. The Activator class The Activator class must be composed exactly in that way, an empty Java Class with @KieActivator annotation. Any additional code can affect negatively the feature behavior. Why are these requisites mandatory? Let’s go deeper to see how it works under the hood. THE JAVA COMPLETION EXTENSION In the last months, we designed and implemented an extension of VSCode plugin. As you can imagine, the main aim of that plugin is to provide language support for Java class files in VSCode. It provides, for instance, the Code completion feature when a user starts to type a Class in the editor. Our extension leverages that plugin and that Code Completion feature to retrieve Java Classes names and field names and eventually translated them to DMN types. Java Completion Extention Architecture In particular, the extension is composed by: * An API Module: There are the entry points of our extension. Currently, the API offers two methods: * getClasses(query: String): Which returns a list of class names that match with a given query term. (eg. Given “Boo” as a query parameter, a possible return list is ["com.Book", "com.Boom", "com.Boomerang", .. ] ) * getAccessors(className: String): Which returns the fields of a given class (eg. Given “Book” as a className parameter, a possible return list is [isAvaialble: boolean, author: com.Author, ...] ) * The Activator Java Class file: This Java class is the place where we simulate a Code Completion request to be served by the Language Support for Java(TM) by Red Hat. For example, in the case of a getClasses("Book") request, the activator will be dynamically filled according to this template: Template used to simulate a Code Completion request Considering our example, that template is filled with the parameter shown in the below screenshot: The template filled with our example parameters As you noticed, the Code Completion correctly suggests the Book class, and this represents the output of the API call too. Please note that this logic doesn’t actually modify the user-defined Activator class file, but it simply sends a request using using the , which is implemented by the . IMPORTING YOUR JAVA CLASSES At this point, we are ready to Import our Java classes as DMN Type, let’s start! Open your DMN model, in our case our reference is the TrafficViolation.dmn file already available in the example project. Go to the "Data Types" tab. Here, you should notice a new button, Import Java Classes. Be aware that this button is enabled only if the previously described requirements are satisfied. Import Java classes button in Data Types section After pressing the button, a pop will appear. Here, you can start to type the name of the class you need to import as DMN type. Let’s type "Book". Selecting Book class As a result of the query (and of the Code Completion behind the scenes), you should see the com.Book class as the query result. Select it and press the "Next" button. Managing the fields of the Book class In this step, you should expect to see all the fields of the com.Book class, with their own DMN type (eg. Book is a structure, name a string, numberOfCopies a number) Let’s pay attention to the first field, author. That field type is another user-defined Java class, com.Author. If you want to import that class too, just press the "Fetch "Author" class". Importing both Author and Book classes As a result, both com.Author and com.Book classes with their own fields are present in the list. Note that author field’s type is now Author. Time to land on the last step, the review one. Here, we should simply review if the selected classes are consistent with our expectations. We are definitely ready to import these DMN Type, so press the "Import" straightaway! Java classes imported as DMN Types And voila! com.Author and com.Book Java classes have been translated to Author and Book DMN types, ready to be used in your DMN logic. CONCLUSION In this article, we showed how to use our new "Import Java Classes" feature to import your domain Java classes as DMN Types, a useful alternative to importing your domain types. Thank you for reading! The post appeared first on .</content><dc:creator>Yeser Amer</dc:creator></entry><entry><title>How to use Operators with AWS Controllers for Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes" /><author><name>August Simonelli</name></author><id>481f182a-39a9-486b-9e0c-8425aec05234</id><updated>2022-05-16T07:00:00Z</updated><published>2022-05-16T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first of two articles that show how to simplify the management of services offered for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; by Amazon Web Services (AWS), through the use of &lt;a href="https://gallery.ecr.aws/aws-controllers-k8s"&gt;Amazon's AWS Controllers for Kubernetes&lt;/a&gt; (ACK). You'll also learn how to use an &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Operator&lt;/a&gt; to simplify installation further on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; clusters. Together, these tools provide standardized and familiar interfaces to AWS services from a Kubernetes environment.&lt;/p&gt; &lt;p&gt;This first article lays out the reasons for using controllers and Operators, and sets up your environment for their use. A subsequent article will show how to install AWS services and how to use them within OpenShift. The ideas behind these articles, and a demo showing their steps, appear in my video &lt;a href="https://www.youtube.com/watch?v=MEKTnbeXv2Y"&gt;Using AWS Controllers for Kubernetes (ACK) with Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;So many services, so little time&lt;/h2&gt; &lt;p&gt;If you deploy any kind of code on AWS, you know that the platform offers plenty of services for your applications to consume. Perhaps you use AWS services to support your application's infrastructure—to create registries with Amazon's Elastic Container Registry (ECR) or compute instances with EC2, for instance. Or maybe you're integrating AWS services directly into your development pipeline, deploying an RDS database on the back end, or building, training, and deploying &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; (ML) models with Amazon SageMaker.&lt;/p&gt; &lt;p&gt;Whatever you are doing, AWS probably has a service to make your work easier, your application better, and your use of time more efficient.&lt;/p&gt; &lt;p&gt;But with so many services, how do you integrate them easily into your code? Do you write complex CloudFormations scripts to call from your &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; workflows? Are you more old school and like to write wrappers in a familiar language? Or maybe it's all about APIs for you? One thing is for sure: There are a lot of options (and a lot of hoops to jump through).&lt;/p&gt; &lt;p&gt;And what if you're running Kubernetes in AWS—maybe on EKS, maybe on OpenShift, or maybe you're just rolling your own? How do you stay within the efficient, familiar, and friendly framework of coding for Kubernetes and still access AWS services without coming up with complex, confusing, and hard-to-maintain workarounds that break your workflow?&lt;/p&gt; &lt;h2&gt;It's easy: You ACK it&lt;/h2&gt; &lt;p&gt;With the growth of Kubernetes for mission-critical production workloads, AWS is a match made in heaven for Kubernetes developers. With so many resilient services, Kubernetes in AWS is a smorgasbord of functionality to improve, scale, and prepare your app for anything. Bring on Black Friday sales and Click Frenzy shopping days—Kubernetes in AWS has cloud-native efficiencies and AWS resiliency built-in.&lt;/p&gt; &lt;p&gt;And now, with AWS Controllers for Kubernetes (ACK), you can easily define and use AWS resources directly from Kubernetes. ACK allows Kubernetes users to define AWS resources using the Kubernetes API. This means you can declaratively define and create an AWS RDS database, S3 bucket, or many other resources, using the same workflow as the rest of your code. There's no need to break out and learn AWS-specific languages or processes. Instead, when an ACK controller is installed, you can use the Kubernetes API to instruct the controller to interact with the AWS service. As a bonus, Kubernetes continues to manage the service for you.&lt;/p&gt; &lt;p&gt;To enable these rich possibilities, each ACK instance is a unique Docker image available in the &lt;a href="https://gallery.ecr.aws/aws-controllers-k8s"&gt;ACK public gallery&lt;/a&gt;. An image is combined with a custom resource definition (CRD), allowing you to easily request custom resources (CR) to define the service and use it within your project. Additionally, integration with AWS's Identity and Access Management (IAM) ensures that all steps and interactions are secure and that role-based access control (RBAC) is managed transparently.&lt;/p&gt; &lt;p&gt;To understand how the controllers are built, including the generation of the artifacts, and to learn about the history of the ACK project, see the AWS blog post &lt;a href="https://aws.amazon.com/blogs/containers/aws-controllers-for-kubernetes-ack/"&gt;Introducing the AWS Controllers for Kubernetes (ACK)&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Operator, Operator, could you be so kind&lt;/h2&gt; &lt;p&gt;Although the default distribution method for ACK employs &lt;a href="https://helm.sh"&gt;Helm charts&lt;/a&gt;, a popular deployment tool on Kubernetes, we'll look at a simpler way to manage controllers through an Operator. Operators are a sleek aid to deployment and life cycle management for Kubernetes services. Publicly available Operators can be downloaded from &lt;a href="https://operatorhub.io"&gt;OperatorHub&lt;/a&gt;, a project started by Red Hat but used by many communities.&lt;/p&gt; &lt;p&gt;Operators, along with the &lt;a href="https://operatorframework.io"&gt;Operator Framework&lt;/a&gt;, make it super easy to install, manage, and maintain Kubernetes resources and applications across OpenShift clusters. The Operator Framework is an open source toolkit designed to manage Operators in an effective, automated, and scalable way.&lt;/p&gt; &lt;p&gt;So it's a no-brainer that we at Red Hat have worked hard to ensure that installing ACK service controllers with Operators is easy. We work closely with AWS engineers to ensure nothing is lost in the process. See &lt;a href="https://cloud.redhat.com/blog/attention-developers-you-can-now-easily-integrate-aws-services-with-your-applications-on-openshift"&gt;Attention developers: You can now easily integrate AWS services with your applications on OpenShift&lt;/a&gt;, a post on the Red Hat Cloud blog, for more details about that collaboration.&lt;/p&gt; &lt;h2&gt;Setup in AWS and Kubernetes&lt;/h2&gt; &lt;p&gt;Before installing a controller via OperatorHub, a cluster administrator needs only to carry out a few simple pre-installation steps in AWS to provide the controller credentials and authentication context for interacting with the AWS API.&lt;/p&gt; &lt;p&gt;Let's take a look at that process now. I'm using an OpenShift installation in AWS provided by &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/aws"&gt;Red Hat OpenShift Service on AWS&lt;/a&gt;, but you can use the Operators on any OpenShift cluster running on AWS with the &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager (OLM)&lt;/a&gt; installed.&lt;/p&gt; &lt;p&gt;Let's keep our installation tidy and create a namespace for our controllers. This is the namespace the Operators will expect to find when you install from OperatorHub, so it's best not to change the name after choosing it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project ack-system Now using project "ack-system" on server "https://api.rosatest.c63c.p1.openshiftapps.com:6443". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we need a user in IAM that can own the controllers and be our service account. We are going to attach our security principals to this account. To maintain clear lines between services, you could create a user for each of your controller Operators: one user for your S3 interactions, another for your RDS interactions, and so on. But to keep things simple for this example, we'll use the same user for all controllers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;$ aws iam create-user --user-name ack-user { "User": { "Path": "/", "UserName": "ack-user", "UserId": "AIDARDQA3BHOTGU5KGN24", "Arn": "arn:aws:iam::1234567890:user/ack-user", "CreateDate": "2022-03-24T01:24:03+00:00" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we need an access key ID and secret access key for this user. Record the &lt;code&gt;AccessKeyId&lt;/code&gt; and &lt;code&gt;SecretAccessKey&lt;/code&gt; strings generated by the following command, because you are going to store them in Kubernetes. Storing the keys allows the controllers to interact programmatically with the AWS resources. We will use these keys in a moment, but make sure to write them down and protect them (don't worry about my security, because my keys have been deleted):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws iam create-access-key --user-name ack-user { "AccessKey": { "UserName": "ack-user", "AccessKeyId": "AKIARDQA3BHOVF4ZSHNW", "Status": "Active", "SecretAccessKey": "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", "CreateDate": "2022-03-24T01:24:24+00:00" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to attach the correct AWS policy for each service to the principal (user) you just created. This procedure grants the user control over the specific, correct, AWS resources.&lt;/p&gt; &lt;p&gt;Grant access by assigning a policy's Amazon Resource Name (ARN) to the user directly. As mentioned, you could attach policies in a one-for-one relationship to unique users (e.g., an EC2 ARN to the &lt;code&gt;ack-ec2-user&lt;/code&gt; user), or all policies to a single user. For our example, you are going to attach all our policies to the same user (&lt;code&gt;ack-user&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Recommended policy ARNs are provided in each service controller's GitHub repository in the &lt;code&gt;config/iam/&lt;/code&gt; directory. For instance, the EC2 controller's policy is in the following file on GitHub:&lt;/p&gt; &lt;pre&gt; https://github.com/aws-controllers-k8s/ec2-controller/blob/main/config/iam/recommended-policy-arn&lt;/pre&gt; &lt;p&gt;OK, let's connect some controllers' policies to our &lt;code&gt;ack-user&lt;/code&gt; service account:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws iam attach-user-policy \ --user-name ack-user \ --policy-arn 'arn:aws:iam::aws:policy/AmazonS3FullAccess' $ aws iam attach-user-policy \ --user-name ack-user \ --policy-arn 'arn:aws:iam::aws:policy/AmazonEC2FullAccess'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These commands grant our &lt;code&gt;ack-user&lt;/code&gt; IAM user access to AWS S3 and EC2. If you want to install additional ACK operators for other AWS services, you need to enable those policies, too.&lt;/p&gt; &lt;p&gt;Next, you need to present the &lt;code&gt;ack-user&lt;/code&gt; secure credentials in a way that can be safely passed to the controller, so it will be allowed to make changes to the AWS service for which it is responsible. These credentials are a Kubernetes Secret and a ConfigMap, defined with some specific information. The naming of these assets is intentional and cannot be varied, or the Operator will not be able to find the credentials.&lt;/p&gt; &lt;p&gt;Place the &lt;code&gt;AccessKeyId&lt;/code&gt; and &lt;code&gt;SecretAccessKey&lt;/code&gt; values you recorded early in a file called &lt;code&gt;secrets.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;AWS_ACCESS_KEY_ID=AKIARDQA3BHOVF4ZSHNW AWS_SECRET_ACCESS_KEY=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&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a secret with these keys in the &lt;code&gt;ack-system&lt;/code&gt; namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic \ --namespace ack-system \ --from-env-file=secrets.txt ack-user-secrets secret/ack-user-secrets created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You also need to set some environment variables for the controllers. Do this by creating a ConfigMap in the &lt;code&gt;ack-system&lt;/code&gt; namespace. Add the following to a file called &lt;code&gt;config.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=ap-southeast-2 ACK_RESOURCE_TAGS=acktagged AWS_ENDPOINT_URL=&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll need to adjust the values to suit your own environment. Most of the values should be obvious, but it's important to keep &lt;code&gt;ACK_WATCH_NAMESPACE&lt;/code&gt; blank so that the controller can properly watch all namespaces. Additionally, you should not rename these variables, as the operators are preconfigured to use them as they are here.&lt;/p&gt; &lt;p&gt;Create the ConfigMap:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create configmap \ --namespace ack-system \ --from-env-file=config.txt ack-user-config configmap/ack-user-config created&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Where to now?&lt;/h2&gt; &lt;p&gt;This article has you set up to use as many AWS services as you want through ACK and OperatorHub. The second article in the series will install EC2 and S3 as examples, and perform an S3 operation from Kubernetes.&lt;/p&gt; &lt;h2&gt;Share your experiences&lt;/h2&gt; &lt;p&gt;If you'd like to help, learn more, or just connect in general, head on over to the &lt;a href="http://kubernetes.slack.com"&gt;Kubernetes Slack channel&lt;/a&gt; and join us in #provider-aws to say hello to the AWS and Red Hat engineers creating the code, various ACK users, and even the occasional blog post author.&lt;/p&gt; &lt;p&gt;We're looking for more good examples of complex deployments created in AWS via ACK. If you've got a deployment you think would be made easier with ACK, or one you've already made better, let us know on the Slack channel or in the comments section below. We might showcase your work in some upcoming articles or videos.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes" title="How to use Operators with AWS Controllers for Kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>August Simonelli</dc:creator><dc:date>2022-05-16T07:00:00Z</dc:date></entry><entry><title type="html">Jakarta EE 10 is on its way with WildFly 27</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-ee-10-is-on-its-way-with-wildfly-27/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-ee-10-is-on-its-way-with-wildfly-27/</id><updated>2022-05-16T06:49:09Z</updated><content type="html">WildFly 27 (Alpha) is now available. On the the top features of this releases is the preview support for some of Jakarta EE 10 features plus several product enhancement. This article launches you on a tour of this new release by focusing on fundamentals. Jakarta EE 10 status in WildFly WildFly 27 Alpha is the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
